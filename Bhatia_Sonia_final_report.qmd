---
title: "Predicting Air Quality Levels in India"
subtitle: |
  | Final Project 
  | Data Science 2 with R (STAT 301-2)
author: "Sonia Bhatia"
date: today

format:
  html:
    toc: true
    embed-resources: true
    
execute:
  echo: false
  warning: false

from: markdown+emoji 
reference-location: margin
citation-location: margin
---



::: {.callout-tip icon=false}

## Github Repo Link

[My GitHub Repo Link](https://github.com/stat301-2-2024-winter/final-project-2-soniab101)

:::

```{r}
#| echo: false
# load packages ----
library(tidyverse)
library(tidymodels)
library(here)
library(naniar)
library(gt)

# handle common conflicts
tidymodels_prefer()

set.seed(1234)

air_data <- read_csv(here("data/city_day.csv"))
```

## Introduction

### Prediction question:

My prediction research question is can we predict the air quality on a certain day based off certain factors such as nitric oxide levels, nitric dioxide, and ammonia levels in the air. My variable of interest is air quality level (AQI variable), and this is a regression problem. I am interested in pursuing this problem because it has many important applications in areas of public health, environmental health, and urban development. For example, poor air quality can have severe health implications, especially for vulnerable populations such as children, the elderly, and those with respiratory conditions. Predicting air quality allows authorities to issue warnings and advisories, helping people take necessary precautions to protect their health.


### Data Source:

This dataset is titled â€™Air Quality Data in India (2015-2020) and is downloaded from Kaggle. The data has been compiled from the Central Pollution Control Board (CPCB) website: [https://cpcb.nic.in/](https://cpcb.nic.in/) which is the official body of Government of India. 


## Data Overview:
I have 16 variables in my dataset, and 29531 observations. There are 3 non-numeric variables, city, date, and aqi_bucket, which I transformed to be factor/categorical variables, and 13 numerical variables. Some of the numeric variables are particulate matter 10 micrometers or less in diameter, carbon monoxide, benzene, ozone, and more. Most of the variables are gases in the air that are key in predicting air quality levels. Additionally, many of these variables, such as ozone and nitrogen oxides, are air pollutants with known associations to adverse health effects on humans.


### Cleaned Data

To clean the dataset, I cleaned the variable names, and removed the missingness from my target variable, air quality index. I also created a new variable that was a log transformation of the air quality index variable, which I will explain the reasoning behind in the following section. Additionally, I extracted the month and year from the date variable because I believed those would be more meaningful in predicting our target variable.


### Missingness

```{r}
#| echo: false
#| label: fig-1
#| fig-cap: "Missingness across variables"

load(here("results/miss_var_plot.rda"))

miss_var_plot

```


I do have missingness issues, as explored in @fig-1 
above. All the variables other than date and city are missing observations, however, for most variables they are just missing a small portion of their observations. I am missing about 15% of the data on my target variable, so I knew I needed to exclude those missing values before continuing with splitting my data. I noticed that I am missing almost 2/3 of the observations for the Xylene variable. The variables with large portions missing may not be as useful as predictor variables. Since my dataset is very large though, I do not believe this missing data will be a significant issue since I still have many valid observations.

### Target Variable Analysis
@fig-2
and @fig-3
below show the distribution of the air quality index variable. The median is around 100, and the data points range from 0 to around 2050. From the histogram, we can see that the mode is about 102. As we can see, the data is right skewed, meaning that the data may violate the assumption of normalcy that some statistical tests we may be interested in performing require. The right skewness also tells us that the median will be a better measure of central tendency than the mean for this dataset. To handle this data for our predictive modeling, I decided it would be best to perform a log transformation to normalize the data.

```{r}
#| echo: false
#| label: fig-2
#| fig-cap: "Distribution of AQI (Histogram)"

air_data |> ggplot(aes(x = AQI)) + geom_histogram(fill = "seagreen", color = "white", bins = 30) +
  scale_x_continuous(limits = c(0,1000)) +
  labs(title = "Air Quality in India", x = "Air Quality Index")

```

```{r}
#| echo: false
#| label: fig-3
#| fig-cap: "Distribution of AQI (Boxplot)"


air_data |> ggplot(aes(x = AQI)) + geom_boxplot(fill = "lightseagreen") +
  labs(title = "Air Quality in India", x = "Air Quality Index")
```



## Methods:

### Data Splitting:
I split my cleaned data into training and testing sets using stratified splitting by my target variable, air quality index transformed to now be on a log10 scale, with 70% of the data as the training data and 30% as the testing. 

### Resampling:
The data was resampled using v-fold cross-validation with 5 folds and 3 repeats. This results in 15 models per fit. I chose this method since it allows for me to have a comprehensive assessment of the model's performance as it evaluates the model on different subsets of the data. K-fold cross-validation can provide a more reliable estimate of model performance compared to a single train-test split, especially when the dataset is limited. I chose this over bootstrap resampling as k-fold cross-validation provides a better balance between bias and variance compared to bootstrap resampling. It uses distinct training and validation sets, which can help in better estimating model performance on unseen data. Also, k-fold cross validation tends to be less prone to overfitting than boostrap resampling. I chose 5 folds and 3 repeats since they are a common combination as a benchmark, and I did not want to slow down my computation time significantly when creating models. 

### Recipes:
I use 4 recipes to create my models. Since my predictor variable is air quality index (AQI) on a log10 scale, I removed the original AQI variable in my basic recipe. I also chose to remove the variable that classified which bucket the AQI index fell into, such as severe or moderate, since that is not used in predicting the AQI but is determined after the fact based off of the value. I chose to remove the date as well since I already extracted the month and year which were more of interest to me. Since many of my variables had missingness, I imputed the nominal predictors by substitutiting missing values of nominal variables by the training set mode of those variables, and imputed the numeric predictors by subtituting the missing values by training set median of these variables. In the future, I would choose to remove some of the variables containing lots of missingess such as xylene because these most likely do not serve as good predictors. Additionally, the recipe creates dummy variables for the nominal predictors, removes the variables that have zero variance, and normalizes all the numeric predictors.

The main recipe does the same steps as the basic recipe, except it creates a variable for season based off of the month variable. I did this to avoid using a variable with too many levels, and I felt like the season itself is what could be having an impact on the AQI. This recipe included interaction terms, which analyzes the impact of these variables separately. I conducted an EDA on my dataset to determine the relationships between my predictor variables. I figured there may be a relationship between the time of year and certain gases that become more concentrated in the atmosphere. I investigated and found that there was a relationship between the season and nitrogen dioxide levels in the air. There are much higher levels in the winter seasons and significantly lower levels in the monsoon season. Additionally, I found a relationship between season and ozone levels in the air. I found a similar trend to carbon monoxide levels through the seasons, and decided to also include it as in interaction term. I thought city and carbon monoxide levels may be related since cities with higher populations would have more traffic flow releasing more carbon monoxide. After investigating these variables, I found a relationship between the two. I ended up not including this interaction because the city variables had so many levels (26) that I figured I could be over parameterizing my recipe. Next, I figured particulate matter of size 2.5 micrometers or less would be related to levels of larger particular matter of size 10 micrometers or less, and I found that there was a positive relationship between the two. The graphs from this EDA can be found in the appendix. 

The basic and main recipes for the forest models include the same steps as these basic and main recipes respectively, except they also utilize one-hot encoding. 


### Model Types:
The problem I am interested in here is a regression problem since I am trying to predict the numeric AQI. The models I created were null, linear, elastic net, k-nearest neighbors, boosted tree, and random forest. The null model only uses the basic parametric recipe, while the lienar and elastic net use the basic and main parametric recipes. The  k-nearest neighbors, boosted tree, and random forest models all use the basic and main tree recipes. 

### Tuning Parameters:
Tuning parameters are set before the training begins. They control the behavior of the learning algorithm and can significantly affect the performance of the model. The purpose of tuning parameters is to optimize the model's performance by finding the best combination of hyperparameters for a given dataset and problem.

The elastic net, boosted tree, random forest, and nearest neighbors models all utilize tuning. 

Firstly, the elastic net model uses tuning for the mixture and penalty. I updated the range of  the mixture to be from 0 to 1, since the lower bound of 0 is the standard value for a fully ridge model and the upper bound of 1 indicates a fully lasso model. With this range, I could determine what the optimal amount of ridge and lasso is. The tuning parameters for penalty have been updated to range from -2 to 0. The range for penalty is on a log10 scale here, so this range once transformed back to a normal scale ranges from 10^-2 = 0.01 to 10^0 = 1. The penalty is typically set at 0.01, so I thought it would be interesting to explore a wider variety of penalties to assess which is the best. I  used 10 levels, so the model was fit to 10 different levels of penalty in the range. 

The random forest model uses tuning for determining the number of sample predictors, minimum node size. I chose the range of the number of sample predictors to be from 0 to 15, since the upper bound is the number of predictors I had. The range for mtry should typically cover a wide enough span to explore different levels of randomness in feature selection. For the minimum node size, I used the default range since that typically contains the best value for that hyperparameter. The minimum node size acts as a stopping criterion for splitting in a decision tree in random forest models. I chose a fixed number of trees of 100. While typically you want to chose a high value for trees as your processor can handle without increasing the computation time too much. As the number of trees grows, it does not always mean the performance of the forest is significantly better than previous forests, so one does not need to always choose an extremely high number of trees. I tried running one random forest model with 100 trees, and found that its rmse was fairly low, so I determined to proceed with using 100 trees to create all of my random forest models since my computation time would not be too long. In the future, I may want to try still using a slightly higher value for the trees to see if it gives me a higher model performance. 


The boosted tree model utilizes tuning parameters for determining the number of sample predictors, minimum node size, and learning rate.  For the minimum node size, I did not update it and kept it as the default range again. I also used the same range for the number of sample predictors. The learning rate means how fast the model learns. Gradient boosted decision trees are typically quick to learn and overfit training data. The learning rate is an effective way to slow down learning. I updated the learning rate range to (-5, -0.2) since -0.2 was close enough to 0, and lower ranges of learning rates are good when using more trees. Looking back on my range selection, since I used a number of trees that was relatively low, I could have maybe increased the learning rate range. This range is also on a log10 scale. 


The k-nearest neighbors model utilizes tuning for the neighbors variable. I did not update the range that it uses, since the default range typically contains the best hyperparameter values and results it the most accurate knn models.  


### Assessment metric
To evaluate the performance of my predictive model, I will use rmse. Originally, I had chosen to also use r^2 as a performance metric, but since some models such as elastic net do not use it as a metric, I figured it would be better to have one standardized metric to compare all my models on. RMSE is the root mean squared error, and tells us how much error our model has in predicting values that correctly match the actual values. I will use RMSE as my primary metric, and determine which model performs the best by evaluating which has the lowest RMSE. The rmse values in the upcoming tables are on the log10 scale. 

## Model Building & Selection Results:

## Model Performance on Basic Recipes:

@tbl-1 
below shows the performance of the six models built on the basic recipe. It is arranged from best performance based off of rmse value to worst performance. This table demonstrates to us that all of the models performed better than the null model on this recipe, meaning that building more complex models is worth the additional computation time and space. There is quite a large drop in rmse from the null's rmse value of 0.29 to the elastic net's rmse of 0.13. From this table, we can see that the random forest model performed the best and had the lowest rmse of 0.08 meaning that its predictions of air quality indices was accurate. 
```{r}
#| echo: false
#| label: tbl-1
#| tbl-cap: "Evaluation of six models built from the basic recipe"

load(here("results/tbl_result_basic.rda"))

tbl_result_basic
```


In @tbl-2
below, we explore the 5 other model types' performance on their main recipes. Once again, the random forest model performed the best with the lowest rmse value of 0.080. All of the models have very low and similar values for the standard error. While the elastic net model had the lowest performance, its rmse was still fairly low with a value of 0.12. All of the models seem to be making accurate predictions in comparison to the actual values, but we can still observe from this table that more complex models are useful in making the most accurate predictions. 


```{r}
#| echo: false
#| label: tbl-2
#| tbl-cap: "Evaluation of six models built from the main recipe"

load(here("results/tbl_result_adv.rda"))

tbl_result_adv
```

### Best Hyperparameters Per Model

#### Random Forest and Boosted Tree (Tree-Based Models)
@tbl-3 and @tbl-4 shows the best hyperparameters for the boosted tree and random forest models. For all models except for the random forest on basic recipe, the optimal number of sample predictors was 15. This demonstrates that it is important to select the range of sample predictors to include the total number of predictors. I found it interesting that the optimal minimal node size was much higher for the boosted tree models than it was for the random forest models. The best learning rate was 0.63, and was the same for the basic and feature engineered model for the boosted tree models. It is interesting that this is the upper bound of our learning rate range (10^-2), and demonstrates that the learning rate should be set close to 0.

```{r}
#| echo: false
#| label: tbl-3
#| tbl-cap: "Best hyperparameters for boosted tree model for both recipes"


load(here("bt_param_table.rda"))

bt_param_table
```

```{r}
#| echo: false
#| label: tbl-4
#| tbl-cap: "Best hyperparameters for random forest model for both recipes"


load(here("rf_param_table.rda"))

rf_param_table
```


### K-Nearest Neighbors:

@tbl-5
below shows the optimal hyperparameter values for the k-nearest neighbors model. It is interesting that there is a smaller value for neighbors for the feature engineered recipe than for the basic recipe. A higher number of neighbors typically leads to prevent overfitting of the data. These numbers seem to be slightly high considering I have 15 predictors, which is interesting.

```{r}
#| echo: false
#| label: tbl-5
#| tbl-cap: "Best hyperparameters for nearest neighbors for both recipes"


load(here("knn_param_table.rda"))

knn_param_table
```

### Elastic Net

@tbl-6
shows the best hyperparameters for the elastic net model. The mixture value for the basic recipe is 0, which indicates that a fully ridge regression model had the highest performance. The main recipe's mixture is 0.11, so there is a bit of a mix of lasso. I would be interested to see the impact on the models' performance using a full lasso regression model. The penalty for both basic and main recipes is 10^0.01 which was on the upper end of our range, indicating that a model typically performs better when we use a more strict penalty that allows for the exclusion of more predictors. This makes sense for my model because I had quite a few variables that had some missingness meaning that they may have not been the best predictors.

```{r}
#| echo: false
#| label: tbl-6
#| tbl-cap: "Best hyperparameters for elastic net for both recipes"


load(here("en_param_table.rda"))

en_param_table
```

### Discussion:
Overall, the random forest model performed the best for both the basic and feature engineered recipe. It performed slightly better on the main recipe than it did on the basic recipe, but the difference is surprisingly very minimal. I was surprised by the lack of improvement when using the feature engineered recipe because I expected the interaction terms would help make the model more accurate. However, the model already had a very high performance on the basic recipe, so it would be difficult to increase its accuracy any higher. The boosted tree model performed the second best on both recipes, with a low rmse value of 0.0899 on the basic recipe and slightly worse rmse of 0.092 on the main recipe. The nearest neighbors model performed the next best, with rmse values that were very close to those of the boosted tree models. This model also did not see improvement from using the main recipe. I found it interesting that the top 3 best performing models were all tree based. The linear model and elastic net model both performed the fourth and fifth best, respectively. They had very similar rmse values, but were only slightly higher than their better performing tree models. I was interested to see that the linear model performed better than the elastic net model, especially since I thought the linear model was more simple, and we utilized tuning on the elastic net model. The model that performed the worst was unsurprisingly the null model. It only used the basic recipe, and served as a good benchmark model with an rmse of 0.29. We could see from the null model's significantly higher rmse value that building more complex models is worthwhile to do since they perform significantly better. 

The lack of improvement between models using the basic versus the feature engineered recipes was surprising to me, and indicated that possibly the interaction terms I chose did not help to improve the models' performance. Also, I had expected categorizing the month variable into seasons would possibly make it a better predictor, but it does not seem to have been the case. One reason why there was not a large improvement could have been because some of the variables had many levels, which could have led to over parameterizing the recipes. 

The model with the lowest rmse value and therefore highest performance is the random forest model on the main recipe. I was not surprised by this, as it typically has high accuracy in its predictions and prevents overfitting well. The high number of trees and utilization of tuning parameters most likely contributed to its highly accurate performance. 


## Final Model Analysis

This section evaluates the best performing (main_recipe) random forest models' performance on the testing data.


### Final Model Assessment

@tbl-7
shows the performance measure for the winning model. The rmse value measures how far predicted values are from observed values in a regression analysis. In other words, how concentrated the data around the line of best fit. A lower rmse value is better, and here we can see that the rmse for our final random forest model is 0.08 which is very low.

```{r}
#| echo: false
#| label: tbl-7
#| tbl-cap: "RMSE for final random forest model"

load(here("results/rmse_pred_log_rf.rda"))

rmse_pred_log_rf
```

### Predicted Versus Actual Results

@fig-4
allows us to visualize the accuracy of the random forest model's predictions. It plots the predicted observations by the observed observations, transformed back to the original scale. As we can see, the datapoints are very highly clustered around the line of best fit, showing the high accuracy of the model. It is interesting that the values of around 500 and lower for aqi are highly concentrated around the line of best fit, while the higher values are a little less accurately predicted and tend to fall more below the line of best fit. This could indicate that the higher values are less frequent since often we do not experience very unhealthy air quality levels, and therefore are harder to predict by the model. Also, the slope of the line of best fit may be accurate for most predictions, but then seems to increase too quickly to accurately predict days with higher unhealthy air quality since it anticipates the levels are higher than they actually are. Overall though, the random forest model was very accurate in its predictions. 

```{r}
#| echo: false
#| label: fig-4
#| fig-cap: "Plot of Predicted Observations vs Actual Observations"

load(here("results/final_plot.rda"))

final_plot
```



## Conclusion:

Overall, I was able to learn a lot in depth about the best practices to go about when building complex machine learning models for optimal performance. From my dataset, I was able to gain more insight regarding what steps to take into consideration when building recipes. Since my data had variables with a lot of missingness, I concluded that imputing these variables, either based off of their mode or median, is a good technique to handle missingness. However, I learned that I should most likely remove the variables that contain greater than 30% of missingness, as they may not be useful predictors and should not be included. I am interested to see the difference in my model performance if I had excluded these. Additionally, I have reflected on different ways to treat categorical variables with many levels. In my dataset, I had a variable for city which coded which city the data was taken from. Once I converted this variable to a factor variable, I realized there were 26 separate levels. In the future, I will definitely consider transforming this variable so that only the top 5 most frequently occuring cities are separate levels and the rest fall into an other group. Too many levels can lead to overfitting, so this could have affected my modelsâ€™ performances. I learned that feature engineering can help us determine which variables are good predictors, and help us refine our recipes to determine which ones to include in the final version. 

Furthermore, I was able to observe the benefits of using tuning for hyperparameters, as it allowed us to observe the modelsâ€™ performances on a variety of hyperparameters, and then select the best combination as our final one. 

I realized after doing more analysis that my data set contained time series data since my data was recorded over a consistent period of time. This is definitely something that is important to assess before getting started with modeling for next time, because it influences what types of models should be used for prediction. For example, the Auto-Regressive Integrated Moving Average (ARIMA) models are widely used for time series data, and are used for linear forecasting. A future project could be building an ARIMA model for this dataset to see how it deals with the time series data better. However, my models still performed well despite my data not being the most suited for this type of project. A project I would also be interested in exploring is creating a classification model for this data to see how accurately it can classify the AQI bucket category with levels such as severe, moderate, and good. 


## References:

Air Quality in India. Found on Kaggle, linked here: [https://www.kaggle.com/datasets/rohanrao/air-quality-data-in-india](https://www.kaggle.com/datasets/rohanrao/air-quality-data-in-india)

Data is from Central Pollution Control Board : [https://cpcb.nic.in/](https://cpcb.nic.in/)

## Appendix:
Below is the EDA I performed on the training dataset to explore any interactions that may be present between predictor variables. These plots help to justify my thought processes behind selecting which interaction terms to use when building my main recipe. 


@fig-5 
shows the relationship between nitrogen dioxide levels and the time of year. I anticipated that there must be some relationship between certain atmospheric gases and the time of year, and decided to investigate this. As we can see, in winter levels of no2 reach their highest of around 200000 while they are relatively low in the monsoon season. They do not change much from summer to post monsoon.
```{r}
#| echo: false
#| label: fig-5
#| fig-cap: "Nitrogen Dioxide Levels by Season"

load(here("results/season_no2.rda"))

season_no2
```
In
@fig-6,
we can see a similar relationship between the seasons and ozone levels. They are very high in the winter season with values of around 225000, and much lower in the monsoon season with values close to 50000. In comparison to the nitrogen dioxide levels however, there is a difference in ozone levels in post monsoon and summer seasons. There is a clear relationship between these predictors. 
```{r}
#| echo: false
#| label: fig-6
#| fig-cap: "Ozone Levels by Season"

load(here("results/season_o3.rda"))

season_o3
```


@fig-7
shows the clear relationship between particular matter of size 2.5 micrometers and lower and particulate matter of 10 micrometers and smaller. These particles pose great risk to health, and are present in the air when air quality levels are unhealthy. Since these are both similar particles but of different sizes, I anticipated there would be a relationship between them and decided to investigate it. The relationship almost looks like a square root function graph.
```{r}
#| echo: false
#| label: fig-7
#| fig-cap: "Nitrogen Dioxide Levels by Season"

load(here("results/pm2_5_pm10.rda"))

pm2_5_pm10
```

Lastly, I wanted to investigate if there was a relationship between carbon monoxide levels and the city they are measured in. @fig-8
depicts this relationship. There does seem to be differences in levels dending on the city, with Ahmedabad having much higher mean carbon monoxide values than other cities. The relationship between these two variables could be caused by having high traffic volume since cars release carbon monoxide gas into the air. I did not decide to include this interaction in my final recipe, however, due to the high numebr of levels of the city variable since I did not want to introduce many new varibles once creating an interaction term.
```{r}
#| echo: false
#| label: fig-8
#| fig-cap: "Nitrogen Dioxide Levels by Season"

load(here("results/co_city.rda"))

co_city
```

